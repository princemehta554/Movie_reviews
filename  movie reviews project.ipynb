{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing data set from nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call', 'it', 'a', 'road', 'trip', 'for', 'the', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the imported data\n",
    "movie_reviews.words(movie_reviews.fileids()[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['it', 'is', 'movies', 'like', 'these', 'that', 'make', ...], 'neg')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating document as a array of tuples containing document and the category \n",
    "documents=[]\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid),category))\n",
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating stopwords and punctuation array\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "punct=list(string.punctuation)\n",
    "stop=stopwords.words('english')\n",
    "stop=stop+punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pos_tag, lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which return pos_tag as require in the format by lemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos_tag(w):\n",
    "    if w.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif w.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif w.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif w.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which lemmatize our documents and also remove stopwords\n",
    "def clean_reviews(words):\n",
    "    output_words=[]\n",
    "    for w in words:\n",
    "        if w.lower() not in stop:\n",
    "            pos=pos_tag([w])\n",
    "            clean_word=lemmatizer.lemmatize(w,pos=get_simple_pos_tag(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling function for data cleaning\n",
    "documents=[(clean_reviews(document),category) for document,category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling documents\n",
    "import random\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting document into train and test\n",
    "training_document=documents[0:1700]\n",
    "testing_document=documents[1700:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602369"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding all the words in a array from train documents to create feature set\n",
    "all_words=[]\n",
    "for doc in training_document:\n",
    "    all_words+=doc[0]\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using top k frequency words as features\n",
    "import nltk\n",
    "frequency=nltk.FreqDist(all_words)\n",
    "k=3000\n",
    "common=frequency.most_common(k)\n",
    "feature_set=[i[0] for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating feature for a doc \n",
    "def feature_dict(doc):\n",
    "    feature_dict={}\n",
    "    words=set(doc)\n",
    "    for w in feature_set:\n",
    "        feature_dict[w]= w in words\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling function for train and test to create feature \n",
    "x_train=[(feature_dict(document),category) for document,category in training_document]\n",
    "x_test=[(feature_dict(document),category) for document,category in testing_document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying naivebayesclassifier\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier=NaiveBayesClassifier.train(x_train)\n",
    "nltk.classify.accuracy(classifier,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import svm\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8833333333333333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying svm\n",
    "svc=SVC()\n",
    "classifier_sklearn=SklearnClassifier(svc)\n",
    "classifier_sklearn.train(x_train)\n",
    "nltk.classify.accuracy(classifier_sklearn,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8733333333333333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pplying randomforest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier()\n",
    "classifier_sklearn1=SklearnClassifier(rfc)\n",
    "classifier_sklearn1.train(x_train)\n",
    "nltk.classify.accuracy(classifier_sklearn1,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a array for the category as Y\n",
    "cat=[category for document,category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a document array which contains words join by space as X\n",
    "text_doc=[\" \".join(document) for document,category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the document\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test=train_test_split(text_doc,cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 7, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec=CountVectorizer(max_features=2000)\n",
    "x_train_feature=count_vec.fit_transform(x_train)\n",
    "x_train_feature.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_feature=count_vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply svm\n",
    "svc2=SVC()\n",
    "svc2.fit(x_train_feature,y_train)\n",
    "svc2.score(x_test_feature,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using countvectorizer with ngram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec2=CountVectorizer(max_features=2000, ngram_range=(2,3))\n",
    "x_train_feature=count_vec2.fit_transform(x_train)\n",
    "x_train_feature.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_feature=count_vec2.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying svm \n",
    "svc2=SVC()\n",
    "svc2.fit(x_train_feature,y_train)\n",
    "svc2.score(x_test_feature,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.32642951, 0.        , ..., 0.        , 0.03186018,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using tfidvectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vec2=TfidfVectorizer(max_features=2000)\n",
    "x_train_feature=count_vec2.fit_transform(x_train)\n",
    "x_train_feature.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_feature=count_vec2.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.812"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying svm\n",
    "svc2=SVC()\n",
    "svc2.fit(x_train_feature,y_train)\n",
    "svc2.score(x_test_feature,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
